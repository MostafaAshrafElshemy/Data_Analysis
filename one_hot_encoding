#!/usr/bin/env python
# coding: utf-8

# <h2>Categorical Variables and One Hot Encoding</h2>

# In[1]:


import pandas as pd


# In[2]:


df = pd.read_csv("homeprices.csv")
df


# In[3]:


df.replace(to_replace=['monroe township','west windsor','robinsville'], value=["Faysal", "El Helmya","Haram"], inplace=True)


# In[5]:


df


# cheapest location --> faysal          
# most expensive --> Helmya 

# faysal ==> 0 
# Haram ==> 2
# Helmya ==> 1 

# <h2 style='color:purple'>Using pandas to create dummy variables</h2>

# In[6]:


dummies = pd.get_dummies(df.town)
dummies


# In[7]:


merged = pd.concat([df,dummies],axis='columns') # concatination of the two dataframes 
merged


# In[8]:


merged.describe()


# In[ ]:


0,1, faysal 
1,0, helmya 
0,0, haram


# In[9]:


final = merged.drop(['town'], axis='columns')
final


# <h3 style='color:purple'>Dummy Variable Trap</h3>

# When you can derive one variable from other variables, they are known to be multi-colinear. Here
# if you know values of california and georgia then you can easily infer value of new jersey state, i.e. 
# california=0 and georgia=0. There for these state variables are called to be multi-colinear. In this
# situation linear regression won't work as expected. Hence you need to drop one column. 

# **NOTE: sklearn library takes care of dummy variable trap hence even if you don't drop one of the 
#     state columns it is going to work, however we should make a habit of taking care of dummy variable
#     trap ourselves just in case library that you are using is not handling this for you**

# In[10]:


final = final.drop(['Haram'], axis='columns')
final


# In[11]:


X = final.drop('price', axis='columns')
X


# In[12]:


y = final.price
y


# In[13]:


from sklearn.linear_model import LinearRegression
model = LinearRegression()


# In[14]:


model.fit(X,y) #training process


# In[15]:


model.predict(X) # 2600 sqr ft home in new jersey


# In[16]:


model.score(X,y)


# In[17]:


model.predict([[2000,0,0]]) # 2000 sqr ft home in Haram


# In[18]:


model.predict([[2800,0,1]]) # 2800 sqr ft home in Faysal


# In[19]:


model.predict([[2000,1,0]]) # 2800 sqr ft home in Helmya


# <h2 style='color:purple'>Using sklearn OneHotEncoder</h2>

# First step is to use label encoder to convert town names into numbers

# In[30]:


from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()


# In[31]:


dfle = df
dfle.town = le.fit_transform(dfle.town)
dfle


# In[32]:


X = dfle[['town','area']].values


# In[33]:


X


# In[34]:


y = dfle.price.values
y


# Now use one hot encoder to create dummy variables for each of the town

# In[35]:


from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
ct = ColumnTransformer([('town', OneHotEncoder(), [0])], remainder = 'passthrough')


# In[36]:


X = ct.fit_transform(X)
X


# In[46]:


X = X[:,1:]


# In[47]:


X


# In[48]:


model.fit(X,y)


# In[49]:


model.predict([[0,1,3400]]) # 3400 sqr ft home in west windsor


# In[35]:


model.predict([[1,0,2800]]) # 2800 sqr ft home in robbinsville


# <h2 style='color:green'>Exercise</h2>


